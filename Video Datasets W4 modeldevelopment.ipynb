{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b4f4122-ea7b-4e67-851e-22278539a5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video features extracted and saved to 'video_features.csv'.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Function to extract features from a video\n",
    "def extract_features_from_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    features = []\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Resize the frame to reduce complexity\n",
    "        frame = cv2.resize(frame, (64, 64))  # Resize to 64x64 pixels\n",
    "        # Flatten the frame and store it as feature vector\n",
    "        feature = frame.flatten()\n",
    "        features.append(feature)\n",
    "    \n",
    "    cap.release()\n",
    "    return np.array(features)\n",
    "\n",
    "# List of video paths\n",
    "video_paths = [\n",
    "    \"C:\\\\Users\\\\sivad\\\\OneDrive\\\\Desktop\\\\viedos\\\\video 1.mp4\",\n",
    "    \"C:\\\\Users\\\\sivad\\\\OneDrive\\\\Desktop\\\\viedos\\\\video 2.mp4\",\n",
    "    \"C:\\\\Users\\\\sivad\\\\OneDrive\\\\Desktop\\\\viedos\\\\video 3.mp4\",\n",
    "    \"C:\\\\Users\\\\sivad\\\\OneDrive\\\\Desktop\\\\viedos\\\\video 4.mp4\",\n",
    "    \"C:\\\\Users\\\\sivad\\\\OneDrive\\\\Desktop\\\\viedos\\\\video 5.mp4\",\n",
    "    \"C:\\\\Users\\\\sivad\\\\OneDrive\\\\Desktop\\\\viedos\\\\video 6.mp4\",\n",
    "    \"C:\\\\Users\\\\sivad\\\\OneDrive\\\\Desktop\\\\viedos\\\\video 7.mp4\",\n",
    "    \"C:\\\\Users\\\\sivad\\\\OneDrive\\\\Desktop\\\\viedos\\\\video 8.mp4\",\n",
    "    \"C:\\\\Users\\\\sivad\\\\OneDrive\\\\Desktop\\\\viedos\\\\video 9.mp4\"\n",
    "]\n",
    "\n",
    "# Extract features from all videos and store them in a list\n",
    "video_features = []\n",
    "for video_path in video_paths:\n",
    "    features = extract_features_from_video(video_path)\n",
    "    video_features.append(np.mean(features, axis=0))  # Taking the average of features from the video\n",
    "\n",
    "# Convert the list of features into a Pandas DataFrame\n",
    "video_features_df = pd.DataFrame(video_features)\n",
    "\n",
    "# Save the extracted features to a CSV file for later use\n",
    "video_features_df.to_csv('video_features.csv', index=False)\n",
    "print(\"Video features extracted and saved to 'video_features.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ff922ec-7bdc-47fe-a54c-7b00acc406e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0           1           2           3           4          5  \\\n",
      "0    9.558376    7.380711    6.807107   10.062606    7.487310   6.382403   \n",
      "1   56.580161   51.666667   63.122261   64.928489   58.710496  70.284890   \n",
      "2  109.174528  131.129717  143.155660   43.919811   50.495283  62.275943   \n",
      "3  192.547101  131.172101  113.099638  189.728261  118.902174  96.554348   \n",
      "4   68.221914   49.718447   56.184466   69.152566   50.603329  58.289875   \n",
      "\n",
      "            6           7          8           9  ...       12279       12280  \\\n",
      "0   14.395939   10.592217   8.050761   11.148900  ...   35.549915   16.126904   \n",
      "1   71.919262   63.198385  76.417532   79.448674  ...   82.681661   90.833910   \n",
      "2   41.634434   49.884434  59.000000   51.195755  ...  162.129717  164.344340   \n",
      "3  192.960145  118.927536  94.802536  194.472826  ...   63.423913  121.927536   \n",
      "4   71.865465   52.467406  61.489598   74.266297  ...   88.703190   45.986130   \n",
      "\n",
      "        12281       12282       12283       12284       12285       12286  \\\n",
      "0   11.556684   27.324873   11.443316    8.060914   23.071066   10.629442   \n",
      "1   98.757785   58.799308   49.071511   45.310265  102.058824   71.407151   \n",
      "2  184.910377  106.120283  105.858491  113.603774  246.957547  247.695755   \n",
      "3  151.219203   49.465580  105.327899  134.708333   62.536232  126.338768   \n",
      "4   37.005548   91.690707   47.682386   38.065187   90.026352   47.428571   \n",
      "\n",
      "        12287  fraud  \n",
      "0    7.389171      1  \n",
      "1   50.680507      0  \n",
      "2  250.915094      1  \n",
      "3  177.657609      1  \n",
      "4   37.081831      0  \n",
      "\n",
      "[5 rows x 12289 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the extracted video features\n",
    "df = pd.read_csv('video_features.csv')\n",
    "\n",
    "# Generate random fraud labels (0 for non-fraud, 1 for fraud) for testing\n",
    "# The number of labels should match the number of videos (9 in this case)\n",
    "df['fraud'] = np.random.randint(0, 2, size=len(df))\n",
    "\n",
    "# Save the labeled dataset to CSV for future use\n",
    "df.to_csv('video_features_with_labels.csv', index=False)\n",
    "\n",
    "# Now you can proceed with training models\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3762a65f-034c-4cd9-aa3d-298ec0537ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0           1           2           3           4          5  \\\n",
      "0    9.558376    7.380711    6.807107   10.062606    7.487310   6.382403   \n",
      "1   56.580161   51.666667   63.122261   64.928489   58.710496  70.284890   \n",
      "2  109.174528  131.129717  143.155660   43.919811   50.495283  62.275943   \n",
      "3  192.547101  131.172101  113.099638  189.728261  118.902174  96.554348   \n",
      "4   68.221914   49.718447   56.184466   69.152566   50.603329  58.289875   \n",
      "\n",
      "            6           7          8           9  ...       12279       12280  \\\n",
      "0   14.395939   10.592217   8.050761   11.148900  ...   35.549915   16.126904   \n",
      "1   71.919262   63.198385  76.417532   79.448674  ...   82.681661   90.833910   \n",
      "2   41.634434   49.884434  59.000000   51.195755  ...  162.129717  164.344340   \n",
      "3  192.960145  118.927536  94.802536  194.472826  ...   63.423913  121.927536   \n",
      "4   71.865465   52.467406  61.489598   74.266297  ...   88.703190   45.986130   \n",
      "\n",
      "        12281       12282       12283       12284       12285       12286  \\\n",
      "0   11.556684   27.324873   11.443316    8.060914   23.071066   10.629442   \n",
      "1   98.757785   58.799308   49.071511   45.310265  102.058824   71.407151   \n",
      "2  184.910377  106.120283  105.858491  113.603774  246.957547  247.695755   \n",
      "3  151.219203   49.465580  105.327899  134.708333   62.536232  126.338768   \n",
      "4   37.005548   91.690707   47.682386   38.065187   90.026352   47.428571   \n",
      "\n",
      "        12287  fraud  \n",
      "0    7.389171      1  \n",
      "1   50.680507      0  \n",
      "2  250.915094      1  \n",
      "3  177.657609      1  \n",
      "4   37.081831      0  \n",
      "\n",
      "[5 rows x 12289 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='video_features_with_labels.csv' target='_blank'>video_features_with_labels.csv</a><br>"
      ],
      "text/plain": [
       "C:\\Users\\sivad\\video_features_with_labels.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import FileLink\n",
    "\n",
    "# Load your DataFrame (assuming you have already saved the file previously)\n",
    "df = pd.read_csv('video_features_with_labels.csv')\n",
    "\n",
    "# Optionally display the DataFrame to check its contents\n",
    "print(df.head())\n",
    "\n",
    "# Create a downloadable link for the CSV file\n",
    "df.to_csv('video_features_with_labels.csv', index=False)\n",
    "\n",
    "# Provide a link to download the CSV file\n",
    "display(FileLink(r'video_features_with_labels.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "476122ac-17a5-466f-93d8-58826fcb3566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution Before Handling Imbalance:\n",
      "fraud\n",
      "1    6\n",
      "0    3\n",
      "Name: count, dtype: int64\n",
      "Class Distribution After SMOTE:\n",
      "fraud\n",
      "1    4\n",
      "0    4\n",
      "Name: count, dtype: int64\n",
      "Logistic Regression Accuracy: 1.0\n",
      "Logistic Regression Precision: 1.0\n",
      "Logistic Regression Recall: 1.0\n",
      "Logistic Regression F1-Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Load the dataset with video features and labels\n",
    "df = pd.read_csv('video_features_with_labels.csv')\n",
    "\n",
    "# Check for class imbalance\n",
    "print(\"Class Distribution Before Handling Imbalance:\")\n",
    "print(df['fraud'].value_counts())\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "X = df.drop('fraud', axis=1)\n",
    "y = df['fraud']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the features to improve model performance\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize SMOTE with k_neighbors=1 to handle the small dataset\n",
    "smote = SMOTE(k_neighbors=1)\n",
    "\n",
    "# Apply SMOTE only to the training set\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Check class distribution after SMOTE\n",
    "print(\"Class Distribution After SMOTE:\")\n",
    "print(pd.Series(y_train_smote).value_counts())\n",
    "\n",
    "# Logistic Regression Model with class_weight set to 'balanced'\n",
    "log_model = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "log_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predict using the test set\n",
    "y_pred_lr = log_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate and print performance metrics\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "precision_lr = precision_score(y_test, y_pred_lr, zero_division=1)\n",
    "recall_lr = recall_score(y_test, y_pred_lr)\n",
    "f1_lr = f1_score(y_test, y_pred_lr)\n",
    "\n",
    "# Output the results\n",
    "print(f'Logistic Regression Accuracy: {accuracy_lr}')\n",
    "print(f'Logistic Regression Precision: {precision_lr}')\n",
    "print(f'Logistic Regression Recall: {recall_lr}')\n",
    "print(f'Logistic Regression F1-Score: {f1_lr}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfe44323-a015-48e4-847c-92c411eef3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 1.0\n",
      "SVM Precision: 1.0\n",
      "SVM Recall: 1.0\n",
      "SVM F1-Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('video_features_with_labels.csv')\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "X = df.drop('fraud', axis=1)\n",
    "y = df['fraud']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize SMOTE with k_neighbors=1 to handle the small dataset\n",
    "smote = SMOTE(k_neighbors=1)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# SVM Model\n",
    "svm_model = SVC(kernel='linear', class_weight='balanced')\n",
    "svm_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "precision_svm = precision_score(y_test, y_pred_svm, zero_division=1)\n",
    "recall_svm = recall_score(y_test, y_pred_svm)\n",
    "f1_svm = f1_score(y_test, y_pred_svm)\n",
    "\n",
    "# Output the results for SVM\n",
    "print(f'SVM Accuracy: {accuracy_svm}')\n",
    "print(f'SVM Precision: {precision_svm}')\n",
    "print(f'SVM Recall: {recall_svm}')\n",
    "print(f'SVM F1-Score: {f1_svm}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9f11e9b-cc92-49cf-97ee-5b0a4d70da04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.6666666666666666\n",
      "Decision Tree Precision: 0.6666666666666666\n",
      "Decision Tree Recall: 1.0\n",
      "Decision Tree F1-Score: 0.8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Decision Tree Model\n",
    "tree_model = DecisionTreeClassifier(class_weight='balanced')\n",
    "tree_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_tree = tree_model.predict(X_test_scaled)\n",
    "\n",
    "accuracy_tree = accuracy_score(y_test, y_pred_tree)\n",
    "precision_tree = precision_score(y_test, y_pred_tree, zero_division=1)\n",
    "recall_tree = recall_score(y_test, y_pred_tree)\n",
    "f1_tree = f1_score(y_test, y_pred_tree)\n",
    "\n",
    "# Output the results for Decision Tree\n",
    "print(f'Decision Tree Accuracy: {accuracy_tree}')\n",
    "print(f'Decision Tree Precision: {precision_tree}')\n",
    "print(f'Decision Tree Recall: {recall_tree}')\n",
    "print(f'Decision Tree F1-Score: {f1_tree}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20a9f2ce-2f3e-4f83-9ef4-f2d94e27bd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Accuracy: 0.6666666666666666\n",
      "CatBoost Precision: 0.6666666666666666\n",
      "CatBoost Recall: 1.0\n",
      "CatBoost F1-Score: 0.8\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# CatBoost Model\n",
    "cat_model = CatBoostClassifier(verbose=0)\n",
    "cat_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_cat = cat_model.predict(X_test_scaled)\n",
    "\n",
    "accuracy_cat = accuracy_score(y_test, y_pred_cat)\n",
    "precision_cat = precision_score(y_test, y_pred_cat, zero_division=1)\n",
    "recall_cat = recall_score(y_test, y_pred_cat)\n",
    "f1_cat = f1_score(y_test, y_pred_cat)\n",
    "\n",
    "# Output the results for CatBoost\n",
    "print(f'CatBoost Accuracy: {accuracy_cat}')\n",
    "print(f'CatBoost Precision: {precision_cat}')\n",
    "print(f'CatBoost Recall: {recall_cat}')\n",
    "print(f'CatBoost F1-Score: {f1_cat}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ffa94d4-0315-4cc2-b039-0293d0be35cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.6666666666666666\n",
      "XGBoost Precision: 1.0\n",
      "XGBoost Recall: 0.5\n",
      "XGBoost F1-Score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# XGBoost Model\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "precision_xgb = precision_score(y_test, y_pred_xgb, zero_division=1)\n",
    "recall_xgb = recall_score(y_test, y_pred_xgb)\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb)\n",
    "\n",
    "# Output the results for XGBoost\n",
    "print(f'XGBoost Accuracy: {accuracy_xgb}')\n",
    "print(f'XGBoost Precision: {precision_xgb}')\n",
    "print(f'XGBoost Recall: {recall_xgb}')\n",
    "print(f'XGBoost F1-Score: {f1_xgb}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44ae806e-8404-418c-9195-73e9848ed079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features which satisfy the provided configuration. Decreasing Dataset parameters min_data_in_bin or min_data_in_leaf and re-constructing Dataset might resolve this warning.\n",
      "[LightGBM] [Info] Number of positive: 4, number of negative: 4\n",
      "[LightGBM] [Info] Total Bins 0\n",
      "[LightGBM] [Info] Number of data points in the train set: 8, number of used features: 0\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "LightGBM Accuracy: 0.3333333333333333\n",
      "LightGBM Precision: 1.0\n",
      "LightGBM Recall: 0.0\n",
      "LightGBM F1-Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# LightGBM Model\n",
    "lgb_model = LGBMClassifier()\n",
    "lgb_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_lgb = lgb_model.predict(X_test_scaled)\n",
    "\n",
    "accuracy_lgb = accuracy_score(y_test, y_pred_lgb)\n",
    "precision_lgb = precision_score(y_test, y_pred_lgb, zero_division=1)\n",
    "recall_lgb = recall_score(y_test, y_pred_lgb)\n",
    "f1_lgb = f1_score(y_test, y_pred_lgb)\n",
    "\n",
    "# Output the results for LightGBM\n",
    "print(f'LightGBM Accuracy: {accuracy_lgb}')\n",
    "print(f'LightGBM Precision: {precision_lgb}')\n",
    "print(f'LightGBM Recall: {recall_lgb}')\n",
    "print(f'LightGBM F1-Score: {f1_lgb}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc6b7ad7-a2df-480b-9971-989fd92b58b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.6666666666666666\n",
      "Random Forest Precision: 1.0\n",
      "Random Forest Recall: 0.5\n",
      "Random Forest F1-Score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest Model\n",
    "rf_model = RandomForestClassifier(class_weight='balanced')\n",
    "rf_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "precision_rf = precision_score(y_test, y_pred_rf, zero_division=1)\n",
    "recall_rf = recall_score(y_test, y_pred_rf)\n",
    "f1_rf = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "# Output the results for Random Forest\n",
    "print(f'Random Forest Accuracy: {accuracy_rf}')\n",
    "print(f'Random Forest Precision: {precision_rf}')\n",
    "print(f'Random Forest Recall: {recall_rf}')\n",
    "print(f'Random Forest F1-Score: {f1_rf}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59f1d8fc-45aa-4cda-826f-136a53a14864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM Accuracy: 0.6666666666666666\n",
      "GBM Precision: 0.6666666666666666\n",
      "GBM Recall: 1.0\n",
      "GBM F1-Score: 0.8\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Gradient Boosting Model\n",
    "gbm_model = GradientBoostingClassifier()\n",
    "gbm_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_gbm = gbm_model.predict(X_test_scaled)\n",
    "\n",
    "accuracy_gbm = accuracy_score(y_test, y_pred_gbm)\n",
    "precision_gbm = precision_score(y_test, y_pred_gbm, zero_division=1)\n",
    "recall_gbm = recall_score(y_test, y_pred_gbm)\n",
    "f1_gbm = f1_score(y_test, y_pred_gbm)\n",
    "\n",
    "# Output the results for GBM\n",
    "print(f'GBM Accuracy: {accuracy_gbm}')\n",
    "print(f'GBM Precision: {precision_gbm}')\n",
    "print(f'GBM Recall: {recall_gbm}')\n",
    "print(f'GBM F1-Score: {f1_gbm}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "35e753be-b7b4-4df3-9885-f41e91a038b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sivad\\Downloads\\jupyter\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 945ms/step - accuracy: 0.5000 - loss: 0.7671\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 0.0118\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 1.0000 - loss: 2.2280e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 1.0000 - loss: 9.3451e-07\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 1.0000 - loss: 1.7262e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 1.8933e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 5.1251e-11\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 6.5401e-07\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.0632e-09\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 2.3621e-05\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sivad\\Downloads\\jupyter\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.6667 - loss: 0.8472\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8333 - loss: 0.2755\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 1.0000 - loss: 0.0792\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 1.0000 - loss: 2.6368e-04\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 3.1505e-04\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 6.7281e-06\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0547\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 9.2726e-07\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 2.7564e-07\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.1566e-08\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sivad\\Downloads\\jupyter\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 0.8787   \n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0730\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0046\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 9.9744e-06\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 5.5153e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 1.7625e-05\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 1.0000 - loss: 1.7350e-06\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 2.7914e-04\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 7.6583e-09\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 2.7833e-07\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sivad\\Downloads\\jupyter\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.4286 - loss: 0.7483\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0441\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0047\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.8571 - loss: 0.1405\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 1.0000 - loss: 0.0110\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 1.0000 - loss: 0.0217\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0784\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.8427e-05\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 3.9138e-07\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sivad\\Downloads\\jupyter\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5714 - loss: 0.8468\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.5714 - loss: 2.8905\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8571 - loss: 0.4365\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.8571 - loss: 0.2971\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8571 - loss: 0.1179\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 7.5012e-04\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8571 - loss: 0.1882\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 1.0000 - loss: 0.0021\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 4.0384e-09\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 1.0000 - loss: 6.1535e-04\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "Cross-Validation Accuracy Scores: [0.5, 0.5, 0.5, 1.0, 1.0]\n",
      "Average Cross-Validation Accuracy: 0.7\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sivad\\Downloads\\jupyter\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - accuracy: 0.5000 - loss: 2.1776\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8750 - loss: 0.8496\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 5.1443e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.8750 - loss: 0.9907\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.7500 - loss: 3.0972\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 0.0022\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0501\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 0.2098\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 1.0000 - loss: 4.2299e-10\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 1.0000 - loss: 1.1304e-07\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Neural Networks Accuracy: 1.0\n",
      "Neural Networks Precision: 1.0\n",
      "Neural Networks Recall: 1.0\n",
      "Neural Networks F1-Score: 1.0\n",
      "Confusion Matrix:\n",
      "[[1 0]\n",
      " [0 2]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('video_features_with_labels.csv')\n",
    "\n",
    "# Prepare features (X) and target (y)\n",
    "X = df.drop('fraud', axis=1)\n",
    "y = df['fraud']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize SMOTE with k_neighbors=1 to handle the small dataset\n",
    "smote = SMOTE(k_neighbors=1)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Define the neural network model\n",
    "def create_nn():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_shape=(X_train_smote.shape[1],)))\n",
    "    model.add(Dropout(0.4))  # Dropout to prevent overfitting, try adjusting to 0.3 or 0.5\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Perform manual cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "accuracy_scores = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_smote):\n",
    "    X_train_cv, X_val_cv = X_train_smote[train_index], X_train_smote[val_index]\n",
    "    y_train_cv, y_val_cv = y_train_smote[train_index], y_train_smote[val_index]\n",
    "\n",
    "    # Create and train the model\n",
    "    model = create_nn()\n",
    "    model.fit(X_train_cv, y_train_cv, epochs=10, batch_size=10, verbose=1)  # Increased epochs to 10\n",
    "\n",
    "    # Evaluate the model\n",
    "    val_predictions = model.predict(X_val_cv)\n",
    "    val_predictions = (val_predictions > 0.5).astype(int)  # Convert to binary predictions\n",
    "    accuracy = accuracy_score(y_val_cv, val_predictions)\n",
    "    accuracy_scores.append(accuracy)\n",
    "\n",
    "# Output the cross-validation accuracy scores\n",
    "print(f'Cross-Validation Accuracy Scores: {accuracy_scores}')\n",
    "print(f'Average Cross-Validation Accuracy: {np.mean(accuracy_scores)}')\n",
    "\n",
    "# Train the model on the full training set with class_weight to balance classes\n",
    "model = create_nn()\n",
    "model.fit(X_train_smote, y_train_smote, epochs=10, batch_size=10, verbose=1, class_weight={0: 1, 1: 3})\n",
    "\n",
    "# Predict using the test set\n",
    "y_pred_nn = model.predict(X_test_scaled)\n",
    "y_pred_nn = (y_pred_nn > 0.5).astype(int)  # Convert probabilities to binary class (0 or 1)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_nn = accuracy_score(y_test, y_pred_nn)\n",
    "precision_nn = precision_score(y_test, y_pred_nn, zero_division=1)\n",
    "recall_nn = recall_score(y_test, y_pred_nn)\n",
    "f1_nn = f1_score(y_test, y_pred_nn)\n",
    "\n",
    "# Output the results\n",
    "print(f'Neural Networks Accuracy: {accuracy_nn}')\n",
    "print(f'Neural Networks Precision: {precision_nn}')\n",
    "print(f'Neural Networks Recall: {recall_nn}')\n",
    "print(f'Neural Networks F1-Score: {f1_nn}')\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_nn)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18a277c-341e-4dfd-88ea-b3f84788afd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
